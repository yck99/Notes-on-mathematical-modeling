# 数据预处理

> [!IMPORTANT]
>
> Ⅰ实际处理过程中很多步骤都是同时进行的，也有些步骤是不需要做的
>
> Ⅱ 提供的python实现由gpt4σ生成，不保证正确性，请自己实践

## 寻找数据
有些题目不会提供数据，这时候可以去一些数据统计网站上自行寻找

## 数据预处理的步骤
### 一. 数据清洗
#### 1. 缺失值检测
顾名思义就是找出哪些数值是空的，一般来说数据量都比较大，主要借助代码完成
在Python中，使用`pandas`库可以很方便地查看数据框中是否存在缺失值。以下是一些常用的方法来检测缺失值：

###### 1. **使用`isnull()`或`isna()`**
`pandas`提供了`isnull()`和`isna()`方法（两者功能相同），用于检测缺失值。如果某个单元格缺失，`isnull()`和`isna()`会返回`True`，否则返回`False`。

```python
import pandas as pd

# 示例数据框
df = pd.DataFrame({
    'A': [1, 2, None, 4],
    'B': [5, None, None, 8],
    'C': [9, 10, 11, 12]
})

# 检查每个单元格是否有缺失值
print(df.isnull())
```

###### 2. **统计每列的缺失值数量**
你可以使用`isnull().sum()`来统计每列中缺失值的数量：

```python
# 统计每列缺失值的数量
print(df.isnull().sum())
```

###### 3. **检查整个数据框是否有缺失值**
使用`any()`结合`isnull()`可以快速检查数据框是否存在缺失值：

```python
# 检查数据框是否存在缺失值
print(df.isnull().values.any())
```

###### 4. **统计整个数据框的缺失值数量**
如果你想知道整个数据框中总共有哪些缺失值，可以使用`sum().sum()`：

```python
# 统计整个数据框的缺失值总数
print(df.isnull().sum().sum())
```

###### 5. **使用`info()`查看数据概览**
`info()`方法会展示每列的数据类型、非空值数量和内存信息，也可以通过这种方式快速查看缺失值的情况：

```python
# 查看数据框的信息，包括每列非空值数量
print(df.info())
```

###### 6. **使用`heatmap`可视化缺失值**
如果想通过图形方式检查缺失值，可以使用`seaborn`库中的`heatmap`函数：

```python
import seaborn as sns
import matplotlib.pyplot as plt

# 可视化缺失值
sns.heatmap(df.isnull(), cbar=False, cmap="viridis")
plt.show()
```

通过这些方法，可以很容易地检测和定位数据框中的缺失值，然后再决定如何处理它们。



#### ==2. 缺失值处理==
##### 内容
- 删除含有缺失值的记录：
 适用于缺失数据量不大且不影响整体数据分布或是数据缺失值占比过大的情况
  包括删除含有缺失值的某一行或是直接删除某一个缺失值过多的特征
  
- 使用平均值、中位数或众数进行填充 :
  ----数据型数据：考虑数据分布情况选择填充策略
  ----分类型数据：通常用众数填充（是否，类型123等等）==也就是插补==
  
  

##### python实现：
在Python中处理缺失值可以通过`pandas`库轻松实现。以下是如何在Python中执行你提到的缺失值处理操作的具体代码示例。

###### 1. **删除含有缺失值的记录**

- 删除含有缺失值的行
使用`dropna()`方法可以删除包含缺失值的行：

```python
import pandas as pd

# 示例数据框
df = pd.DataFrame({
    'A': [1, 2, None, 4],
    'B': [5, None, None, 8],
    'C': [9, 10, 11, 12]
})

# 删除包含任何缺失值的行
df_cleaned = df.dropna()

print(df_cleaned)
```

- 删除含有缺失值的列
如果某列的缺失值较多，使用`dropna()`删除整个列：

```python
# 删除包含任何缺失值的列
df_cleaned = df.dropna(axis=1)

print(df_cleaned)
```



###### 2. **使用平均值、中位数或众数进行填充**

- 使用平均值填充数值型数据
对于数值型数据，可以使用`fillna()`方法填充缺失值：

```python
# 使用平均值填充
df['A'] = df['A'].fillna(df['A'].mean())

print(df)
```

- 使用中位数填充数值型数据
如果数据存在明显的异常值，中位数可能比平均值更稳健：

```python
# 使用中位数填充
df['A'] = df['A'].fillna(df['A'].median())

print(df)
```

- 使用众数填充分类数据
对于分类数据（如类别、布尔值等），可以用众数填充：

```python
# 使用众数填充
df['B'] = df['B'].fillna(df['B'].mode()[0])

print(df)
```



###### 3. **条件删除/填充**

如果想根据特定条件删除或填充，可以使用`pandas`的筛选功能。例如，仅删除某一列缺失值较多的行：

```python
# 只删除列B中有缺失值的行
df_cleaned = df.dropna(subset=['B'])

print(df_cleaned)
```

这些是`pandas`中处理缺失值的基本方法。根据数据的具体情况，可以选择最合适的方法来清理和填充缺失值。



#### 2. 插值方法优缺点

##### （1）拉格朗日插值(Lagrange Interpolation):
- 拉格朗日插值是基于Lagrange多项式来估计缺失值的方法。对于给定的n+1个数据点，可以构建一个n阶的多项式来估计任何中间点的值。
###### 优点：
1. 公式简单，容易理解和实现。
2. 对于给定的数据点，只有一个插值多项式。
###### 缺点：
1. 当数据点增加时，计算量随之增加。

2. 对于高阶插值，可能会出现Runge[^1]现象，即多项式在边界处出现较大的振荡。

3. 添加新的数据点需要重新计算整个插值多项式。

   
##### （2）牛顿插值(Newton Interpolation):
- 牛顿插值是基于已知数据点的差商来构建插值多项式的方法。与Lagrange插值不同，牛顿插值的多项式可以逐步构建，方便添加新的数据点。
###### 优点：
1. 插值多项式可以逐步构建，方便添加新的数据点。
2. 插值多项式的形式可以为嵌套形式，有时更加紧凑。
###### 缺点：
1. 计算差商可能会增加计算复杂性。

2. 与拉格朗日插值相似，高阶插值可能也会出现Runge[^1]现象。

   

##### （3）分段插值(Piecewise Interpolation):
- 分段插值通常使用低阶多项式（如线性或二次多项式）在每个数据段之间进行插值。
###### 优点：
1. 由于使用低阶多项式，避免了高阶插值可能出现的Runge现象。
2. 更加灵活，可以针对每个数据段选择不同的插值方法。
3. 在数据点集中的区域，插值误差较小。
###### 缺点：
1. 可能会在数据点之间产生不连续。

2. 需要为每个数据段存储和计算其插值多项式。

##### 此外还有别的插值方法及其相关python实现，详见 [插值方法及其python实现](/插值方法及其python实现.md)


#### ==3. 异常值检测与处理==
- ##### 基于统计方法的异常值检测：
-3σ原则、等方法可以帮助识别潜在的异常值。
—适用于数据分布接近正态分布的情况。

**Python实现**:

```python
import numpy as np

# 示例数据
data = np.array([10, 12, 14, 15, 15, 16, 16, 18, 19, 20, 30])

# 计算均值和标准差
mean = np.mean(data)
std = np.std(data)

# 3σ范围
lower_bound = mean - 3 * std
upper_bound = mean + 3 * std

# 检测异常值
outliers = data[(data < lower_bound) | (data > upper_bound)]

print("3σ范围:", lower_bound, "-", upper_bound)
print("检测到的异常值:", outliers)
```



- ##### 基于可视化的异常值检测：

-箱线图

箱线图是一种常用的可视化工具，可以帮助检测异常值。它显示了数据的分布情况，包括中位数、四分位数和可能的异常值。通常，数据的1.5倍四分位距（IQR）以外的数据点被视为异常值。

一适用于大多数情况，不要求数据服从特定的分布。

**Python实现**:

```python
import matplotlib.pyplot as plt
import seaborn as sns

# 示例数据
data = [10, 12, 14, 15, 15, 16, 16, 18, 19, 20, 30]

# 绘制箱线图
plt.figure(figsize=(8, 6))
sns.boxplot(data=data)

plt.title("箱线图示例")
plt.show()
```

在箱线图中，盒子的上边缘和下边缘分别表示数据的上四分位数（Q3）和下四分位数（Q1），中间的线表示中位数。盒子之外的“胡须”表示Q1 - 1.5 * IQR和Q3 + 1.5 * IQR的范围，超出这个范围的数据点即为异常值。

##### 异常值修正或删除：

一等同缺失值处理

**Python实现示例**:

```python
data = np.array([10, 12, 14, 15, 15, 16, 16, 18, 19, 20, 30])

# 删除异常值
data_no_outliers = data[(data >= lower_bound) & (data <= upper_bound)]

# 替换异常值（用均值替换）
data_mean_replaced = np.where((data < lower_bound) | (data > upper_bound), mean, data)

print("删除异常值后的数据:", data_no_outliers)
print("替换异常值后的数据:", data_mean_replaced)
```

#### 4. 去除重复数据
- 根据所有列或指定列检测重复记录：
  -使用特定的数据标识符或组合列检测重复。
  -确保数据的完整性和一致性。

  #### 4.1.1 **根据所有列检测重复记录**

  - 如果数据集中每一行都代表一个独立的实体，那么可以根据所有列的值来检测重复记录。

  **Python实现**:

  ```python
  import pandas as pd
  
  # 示例数据
  data = {
      'ID': [1, 2, 3, 4, 5, 3],
      'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Charlie'],
      'Age': [25, 30, 35, 40, 45, 35]
  }
  
  df = pd.DataFrame(data)
  
  # 检测重复数据
  duplicates = df.duplicated()
  print("所有列的重复记录:\n", df[duplicates])
  ```

  #### 4.1.2 **根据指定列检测重复记录**

  - 如果某些列可以唯一标识数据（如ID、组合的姓名和年龄等），可以仅根据这些列来检测重复记录。

  **Python实现**:

  ```python
  # 根据指定列检测重复数据
  duplicates_specific = df.duplicated(subset=['Name', 'Age'])
  print("指定列的重复记录:\n", df[duplicates_specific])
  ```

- 删除重复记录：
  -根据业务逻辑选择保留哪些记录。
  一保持数据集的简洁性。

  #### 4.2.1 **删除所有列都相同的重复记录**

  - 默认情况下，`drop_duplicates()`会删除所有列都相同的重复记录，并保留第一次出现的记录。

  **Python实现**:

  ```python
  # 删除所有列都相同的重复记录
  df_no_duplicates = df.drop_duplicates()
  print("去除所有列重复后的数据:\n", df_no_duplicates)
  ```

  #### 4.2.2 **根据指定列删除重复记录**

  - 可以根据指定的列来删除重复记录，保留首次出现的记录。

  **Python实现**:

  ```python
  # 根据指定列删除重复记录
  df_no_duplicates_specific = df.drop_duplicates(subset=['Name', 'Age'])
  print("根据指定列去除重复后的数据:\n", df_no_duplicates_specific)
  ```

  #### 4.2.3 **保留最后出现的记录**

  - 通过设置参数`keep='last'`，可以保留最后出现的重复记录，而不是首次出现的记录。

  **Python实现**:

  ```python
  # 保留最后出现的重复记录
  df_keep_last = df.drop_duplicates(keep='last')
  print("保留最后出现的重复记录后的数据:\n", df_keep_last)
  ```

------



### 二. 特征工程

特征工程是数据科学和机器学习中的核心步骤之一，它涉及从原始数据中提取有用的信息，并通过特征转换、组合或选择，创建更适合模型学习的特征。这一步骤直接影响模型的性能和预测准确性。

#### 1. **特征提取**

特征提取是从原始数据中提取出新的特征。这可以通过从已有的变量中派生出新的特征，或使用特定的方法生成新的特征。

- **时间特征**：从时间戳提取年份、月份、星期几、季度、小时等特征。例如，"日期"列可以拆分为 "年"、"月"、"日"、"星期几"。
- **文本特征**：从文本数据中提取词袋、TF-IDF、N-grams 等特征。
- **统计特征**：对数据进行统计分析，如均值、方差、最大值、最小值等特征的提取。



#### ==2. **特征转换**==

特征转换是对已有的特征进行某种变换，使其更适合模型的学习。例如：

- **标准化/规范化/归一化**：将特征缩放到相同的范围，标准化通常使数据均值为0，方差为1；归一化将数据缩放到[0,1]区间。

  “标准化”“规范化”和“归一化”在数据处理中的含义虽然有时会混淆，但它们通常指代不同的技术或方法：

  ##### 1. **标准化 (Standardization)**

  - **定义**: 标准化是将数据转换为均值为0、标准差为1的分布。它适用于数据接近正态分布的情况，通过这种变换，可以消除原始数据的单位和量纲，使不同特征的数据具有可比性。
  - **常用方法**: Z-score标准化（即标准差标准化）。
    
    **公式**:
    $$
    X' = \frac{X - \mu}{\sigma}
    $$
    其中，$$X $$ 是原始数据，$\ \mu $ 是均值， $\ \sigma $是标准差。

  - **应用场景**: 标准化适用于需要假设数据符合正态分布的算法，比如线性回归、逻辑回归、支持向量机等。

  ##### 2. **规范化 (Normalization)**

  - **定义**: “规范化”有时是标准化的同义词，但更常见的定义是将数据调整到一个特定的范围内（例如 [0, 1] 或 [-1, 1]），这主要用于消除特征的量纲差异，使得不同特征的数据可以在同一个范围内进行比较。
  - **常用方法**: Min-Max规范化、Decimal Scaling。

    **Min-Max 规范化公式**:
    $$
    X' = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
    $$
    
  - **应用场景**: 规范化适用于要求输入数据在一个特定范围内的算法，比如神经网络、K-均值聚类等。

  ##### 3. **归一化 (Normalization)**

  - **定义**: 归一化通常指将数据的某种度量（如向量的L1或L2范数）调整为1。归一化特别适用于需要计算特征向量之间相似度的算法，比如文本处理中的余弦相似度计算。
  - **常用方法**: L1归一化、L2归一化。

    **L2归一化公式**:
    $$
    X' = \frac{X}{\|X\|_2} = \frac{X}{\sqrt{\sum{X_i^2}}}
    $$
    其中，$ \|X\|_2 $ 表示向量的L2范数。

  - **应用场景**: 归一化适用于文本分类、推荐系统等领域。

  ##### **总结**

  - **标准化**: 侧重于调整数据的分布形状，通常转化为均值为0、标准差为1的正态分布。
  - **规范化**: 侧重于将数据调整到一个固定的范围内，如 [0, 1]。
  - **归一化**: 侧重于调整向量的长度，使其范数为1，通常用于计算相似度的场景。

  虽然在某些文献或实践中，这些术语可能会被混用，但理解它们各自的定义和应用场景，可以帮助你在实际数据处理中选择合适的方法。

  - Decimal Scaling:
    通过移动小数点位置来调整数据的范围，适用于数据的数量级差异较大的情况。
    
    

- **对数变换**：对特征取对数，常用于处理长尾分布的数据。

  

- **多项式特征**：生成多项式特征（如平方项、立方项）以捕捉非线性关系。

  

- **Box-Cox 或 Yeo-Johnson 变换**：用于将非正态分布的数据转换为接近正态分布，以便更好地应用线性模型或其他统计分析方法。这两种变换的目标是使数据更加接近正态分布，减少方差，增强模型的预测性能。

#### 3. **特征组合**

特征组合是通过组合已有特征来生成新的特征，帮助模型捕捉复杂的关系。

- **交叉特征**：组合多个特征，生成交叉特征。例如，将`城市人口`与`GDP`相乘，生成一个新的特征，表示人均GDP。
- **聚合特征**：对某个特征在特定分组内进行聚合计算，如计算平均值、总和、计数等。

#### ==4. **特征选择**==

##### 4.1 **概述**

特征选择是从已有的特征集中挑选出最具代表性的特征，以减少模型复杂度，防止过拟合。

特征选择是机器学习和数据分析中的一个重要步骤，旨在从大量特征中选择对模型性能最有影响的特征。通过特征选择，能够提高模型的准确性，减少模型的复杂性，并且在某些情况下可以降低过拟合的风险。

##### 4.2. **特征选择的方法**

###### 4.2.1 **过滤法 (Filter Method)**

- **概念**: 过滤法基于统计指标来选择特征，不依赖于具体的学习算法。常见的过滤方法包括方差阈值法、卡方检验、皮尔逊相关系数等。

- **示例**:
  - **方差阈值法**: 选择方差较大的特征，忽略方差接近于零的特征。
  - **相关系数法**: 选择 与目标变量高度相关的特征。

**Python实现**:

```python
from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2
from sklearn.datasets import load_iris
import pandas as pd

# 加载数据集
iris = load_iris()
X = iris.data
y = iris.target

# 方差阈值法
selector = VarianceThreshold(threshold=0.1)
X_variance = selector.fit_transform(X)

# 卡方检验
X_chi2 = SelectKBest(chi2, k=2).fit_transform(X, y)

print("方差选择后的特征:\n", X_variance)
print("卡方选择后的特征:\n", X_chi2)
```

###### 4.2.2 **包裹法 (Wrapper Method)**

- **概念**: 包裹法将特征选择过程直接与模型训练结合，通过不同的特征组合进行模型训练，然后评估模型性能，选择最优特征子集。常见的方法包括递归特征消除（RFE）。

- **示例**:
  - **递归特征消除 (RFE)**: 递归地训练模型并每次删除最不重要的特征，直到剩下所需的特征数量。

**Python实现**:

```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# 创建随机森林分类器
model = RandomForestClassifier()

# 递归特征消除
selector = RFE(model, n_features_to_select=2, step=1)
selector = selector.fit(X, y)

# 输出选中的特征
print("RFE选中的特征:\n", selector.support_)
```

###### 4.2.3 **嵌入法 (Embedded Method)**

- **概念**: 嵌入法将特征选择过程嵌入到模型训练过程中。模型在训练时会自动选择特征，常见的算法包括Lasso回归、决策树等。

- **示例**:
  - **Lasso回归**: 利用L1正则化，会将某些特征的系数收缩为0，从而达到特征选择的效果。
  - **基于树模型的重要性评分**: 决策树和随机森林模型可以基于特征重要性来选择特征。

**Python实现**:

```python
from sklearn.linear_model import Lasso
from sklearn.feature_selection import SelectFromModel

# 创建Lasso模型
lasso = Lasso(alpha=0.1)

# 基于Lasso选择特征
model = SelectFromModel(lasso)
X_lasso = model.fit_transform(X, y)

print("Lasso选择后的特征:\n", X_lasso)
```

##### 4.3. **特征选择的流程**

1. **理解数据**: 首先要对数据的特征和目标变量有清晰的理解，确定哪些特征可能与目标变量相关。
2. **初步筛选**: 可以使用过滤法进行初步筛选，去除一些明显无关的特征。
3. **模型结合**: 采用包裹法或嵌入法结合模型来选择重要特征。
4. **模型评估**: 使用选出的特征训练模型，并评估模型性能。如果性能不佳，可以重新调整特征选择的策略。

##### 4.4. **常见的注意事项**

- **多重共线性**: 在特征选择中应注意多重共线性问题，即某些特征之间的高度相关性，这可能会影响模型的稳定性。
- **模型类型**: 不同的模型对特征的选择要求不同。例如，线性模型对噪声特征较为敏感，而树模型则更为稳健。
- **特征数量**: 特征数量过少可能导致欠拟合，而过多则可能导致过拟合。需要根据具体问题选择合适的特征数量。

##### 总结

特征选择是提升模型性能和简化模型的重要步骤。通过使用过滤法、包裹法或嵌入法，可以有效地选择出对模型最有贡献的特征。在实际应用中，特征选择需要结合数据的特点和模型的需求，反复进行调整和优化。

#### 5. **类别编码**

类别编码是将类别型变量转换为数值型变量的过程。

- **One-Hot Encoding**：将每个类别转换为一个二进制向量，适用于类别数较少的情况。
- **Label Encoding**：将每个类别转换为一个整数值，适用于有序类别的情况。
- **目标编码（Target Encoding）**：将类别转换为目标变量的统计量（如均值），适用于类别数量多且有规律的情况。

#### 6. **[降维](/降维.md)**

降维是将高维数据映射到低维空间，减少特征数量。

- **[PCA](https://blog.csdn.net/MoreAction_/article/details/107463336)（主成分分析）**：通过线性变换，将数据投影到主成分空间，减少特征维度。
- **LDA（线性判别分析）**：类似于PCA，但考虑了类标签信息，适用于分类任务。

#### 7. **特征构造**

特征构造是根据业务知识或领域知识创建新的特征。

- **领域知识**：根据行业经验或业务背景，创建可能对预测有帮助的新特征。
- **交互特征**：结合业务理解，构建能够捕捉特定交互关系的特征。

#### 8. **特征生成**

特征生成是指通过自动化的方式，使用算法生成新的特征。

- **特征工具**：使用工具（如`FeatureTools`）自动生成衍生特征。
- **深度学习**：通过神经网络自动学习复杂的特征表示。

#### 总结

特征工程的目标是通过合理的特征处理，提升模型的表现。一个好的特征工程过程，可以使复杂问题变得简单，同时提高模型的预测能力。特征工程在实践中高度依赖对数据和业务问题的深刻理解，因此需要结合具体的数据和任务进行个性化处理。





### 三.数据集成
将多个数据合并成一个数据，例如我们得知了一个矩形的长和宽，那么我们就不需要长宽比这个数据了





[^1]: **Runge现象**是指在使用高次多项式插值函数进行插值时，特别是对等距节点插值，插值多项式在区间边界附近会出现显著的振荡或误差急剧增大的现象。这个现象最早由德国数学家Carl David Tolmé Runge在1901年发现，因此得名。现象描述Runge在研究时发现，在区间`[-1, 1]`上，使用等距节点进行高次多项式插值来逼近函数时，随着多项式次数的增加，插值函数在区间边界附近的振荡性增加，插值误差反而变大，尽管在节点处插值函数仍然能够完美通过数据点。这种现象表明，高次多项式并不总是良好的插值函数选择。Runge 例子Runge使用的一个典型例子是函数：f(x)=11+25x2f(x) = \frac{1}{1 + 25x^2}f(x)=1+25x21在区间`[-1, 1]`上进行插值。产生原因Runge现象的产生主要由于高次多项式的灵活性太大，在节点之间可能会产生过度的振荡，特别是在靠近区间边界时。这种振荡使得多项式在区间边界附近的逼近效果变差，导致误差急剧增加。



